{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text classification in Python using Tensorflow**\n",
    "## **Written by:** Aarish Asif Khan\n",
    "## **Date:** 13 January 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Fetching and creating a model on the IMDB dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# Load the IMDb dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, num_words=10000 means you want to keep only the top 10,000 most frequently occurring words in the dataset, discarding less common words. Adjust this parameter based on your specific needs.\n",
    "\n",
    "After loading the data, x_train and x_test contain lists of movie reviews, where each review is a list of integers representing word indices. y_train and y_test contain the corresponding sentiment labels (0 for negative, 1 for positive).\n",
    "\n",
    "Keep in mind that the IMDb dataset from Keras is already preprocessed, and the reviews are converted into sequences of word indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **If you want to convert the indices back to words for readability, you can use the following code snippet:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1641221/1641221 [==============================] - 1s 1us/step\n",
      "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in x_train[0]])\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # **Creating a ML model with IMDB dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a basic example of how you can create a simple neural network model for sentiment analysis using the IMDb dataset in Python with TensorFlow/Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\User\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:From c:\\Users\\User\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\User\\anaconda3\\envs\\tf_env\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "391/391 [==============================] - 117s 289ms/step - loss: 0.4199 - accuracy: 0.8102 - val_loss: 0.3210 - val_accuracy: 0.8645\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 106s 272ms/step - loss: 0.2381 - accuracy: 0.9098 - val_loss: 0.3255 - val_accuracy: 0.8628\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 102s 260ms/step - loss: 0.1939 - accuracy: 0.9288 - val_loss: 0.3249 - val_accuracy: 0.8762\n",
      "782/782 [==============================] - 47s 60ms/step - loss: 0.3249 - accuracy: 0.8762\n",
      "Test Loss: 0.3249, Test Accuracy: 0.8762\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "# Load the IMDb dataset\n",
    "max_features = 10000  # Only consider the top 10,000 words\n",
    "maxlen = 500  # Cut reviews after 500 words\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Explaining the code above**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data: We load the IMDb dataset and limit it to the top 10,000 words (max_features). We also limit the length of each review to 500 words (maxlen).\n",
    "\n",
    "Preprocessing Data: We pad sequences to make sure they all have the same length.\n",
    "\n",
    "Creating the Model:\n",
    "\n",
    "We use an Embedding layer to convert word indices to vectors.\n",
    "An LSTM layer processes the sequence of vectors.\n",
    "Finally, a Dense layer with a sigmoid activation function is used for binary classification (positive or negative sentiment).\n",
    "Compiling the Model: We specify the optimizer, loss function, and metrics.\n",
    "\n",
    "Training the Model: We train the model on the training data and validate it on the test data.\n",
    "\n",
    "Evaluating the Model: We evaluate the model on the test set and print the test loss and accuracy.\n",
    "\n",
    "This is a basic example, and you may need to adjust hyperparameters, model architecture, or use more advanced techniques depending on your specific requirements and the performance you aim to achieve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
